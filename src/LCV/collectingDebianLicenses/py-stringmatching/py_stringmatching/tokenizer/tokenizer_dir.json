{
    "content": [
        {
            "hidden": false,
            "name": "__init__.py",
            "stat": {
                "perms": "rw-r--r--",
                "size": 0,
                "symlink_dest": null,
                "type": "-"
            },
            "type": "file"
        },
        {
            "hidden": false,
            "name": "alphabetic_tokenizer.py",
            "stat": {
                "perms": "rw-r--r--",
                "size": 1818,
                "symlink_dest": null,
                "type": "-"
            },
            "type": "file"
        },
        {
            "hidden": false,
            "name": "alphanumeric_tokenizer.py",
            "stat": {
                "perms": "rw-r--r--",
                "size": 1974,
                "symlink_dest": null,
                "type": "-"
            },
            "type": "file"
        },
        {
            "hidden": false,
            "name": "definition_tokenizer.py",
            "stat": {
                "perms": "rw-r--r--",
                "size": 745,
                "symlink_dest": null,
                "type": "-"
            },
            "type": "file"
        },
        {
            "hidden": false,
            "name": "delimiter_tokenizer.py",
            "stat": {
                "perms": "rw-r--r--",
                "size": 3653,
                "symlink_dest": null,
                "type": "-"
            },
            "type": "file"
        },
        {
            "hidden": false,
            "name": "qgram_tokenizer.py",
            "stat": {
                "perms": "rw-r--r--",
                "size": 8314,
                "symlink_dest": null,
                "type": "-"
            },
            "type": "file"
        },
        {
            "hidden": false,
            "name": "tokenizer.py",
            "stat": {
                "perms": "rw-r--r--",
                "size": 939,
                "symlink_dest": null,
                "type": "-"
            },
            "type": "file"
        },
        {
            "hidden": false,
            "name": "whitespace_tokenizer.py",
            "stat": {
                "perms": "rw-r--r--",
                "size": 2129,
                "symlink_dest": null,
                "type": "-"
            },
            "type": "file"
        }
    ],
    "directory": "tokenizer",
    "package": "py-stringmatching",
    "path": "py-stringmatching/0.4.2+git20201204.6a7fb57-7/py_stringmatching/tokenizer",
    "pkg_infos": {
        "area": "main",
        "copyright": true,
        "ctags_count": 0,
        "license": "/copyright/license/py-stringmatching/0.4.2+git20201204.6a7fb57-7/",
        "metric": {
            "size": 1664
        },
        "pts_link": "https://tracker.debian.org/pkg/py-stringmatching",
        "sloc": [
            [
                "python",
                4001
            ],
            [
                "makefile",
                174
            ],
            [
                "sh",
                7
            ]
        ],
        "suites": [
            "bookworm",
            "sid"
        ],
        "vcs_browser": "https://salsa.debian.org/python-team/packages/py-stringmatching",
        "vcs_type": "git"
    },
    "type": "directory",
    "version": "0.4.2+git20201204.6a7fb57-7"
}